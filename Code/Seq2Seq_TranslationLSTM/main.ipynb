{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: list of source and target sentences\n",
    "source_sentences = ['I am a student.', 'I love programming.', 'How are you?']\n",
    "target_sentences = ['Je suis étudiant.', 'J\\'aime la programmation.', 'Comment ça va?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokenizer = Tokenizer()\n",
    "source_tokenizer.fit_on_texts(source_sentences)\n",
    "source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
    "source_sequences = pad_sequences(source_sequences, padding='post')\n",
    "\n",
    "target_sentences = ['<start> ' + sentence + ' <end>' for sentence in target_sentences]\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_sentences)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "target_sequences = pad_sequences(target_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(source_sequences, target_sequences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, x, enc_output, state_h, state_c):\n",
    "        x = self.embedding(x)\n",
    "        dec_output, dec_state_h, dec_state_c = self.lstm(x, initial_state=[state_h, state_c])\n",
    "        output = self.fc(dec_output)\n",
    "        return output, dec_state_h, dec_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_src = len(source_tokenizer.word_index) + 1\n",
    "vocab_size_tgt = len(target_tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "\n",
    "encoder = Encoder(vocab_size_src, embedding_dim, units)\n",
    "decoder = Decoder(vocab_size_tgt, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.not_equal(real, 0)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src_seq, tgt_seq):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(src_seq)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tgt_seq[:, :-1]  # Exclude last token for decoder input\n",
    "        real = tgt_seq[:, 1:]  # Actual target sequence\n",
    "        pred, dec_hidden_h, dec_hidden_c = decoder(dec_input, enc_output, dec_hidden_h, dec_hidden_c)\n",
    "        loss = loss_function(real, pred)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0\n",
      "Epoch 2, Loss: 0\n",
      "Epoch 3, Loss: 0\n",
      "Epoch 4, Loss: 0\n",
      "Epoch 5, Loss: 0\n",
      "Epoch 6, Loss: 0\n",
      "Epoch 7, Loss: 0\n",
      "Epoch 8, Loss: 0\n",
      "Epoch 9, Loss: 0\n",
      "Epoch 10, Loss: 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in range(len(X_train) // batch_size):\n",
    "        batch_X = X_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        batch_y = y_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        batch_loss = train_step(batch_X, batch_y)\n",
    "        total_loss += batch_loss\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: je va la programmation\n",
      "Reference: start comment ça va end\n",
      "BLEU Score: 0.04753271977233425\n",
      "Perplexity: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_sentence = X_test[0:1]\n",
    "enc_output, enc_hidden_h, enc_hidden_c = encoder(test_sentence)\n",
    "dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "dec_input = np.array([[target_tokenizer.word_index['start']]])\n",
    "\n",
    "\n",
    "predicted_sentence = []\n",
    "for t in range(y_test.shape[1]):\n",
    "    pred, dec_hidden_h, dec_hidden_c = decoder(dec_input, enc_output, dec_hidden_h, dec_hidden_c)\n",
    "    pred_id = np.argmax(pred[0, -1, :])\n",
    "    predicted_sentence.append(pred_id)\n",
    "    if pred_id == target_tokenizer.word_index['end']:\n",
    "        break\n",
    "    dec_input = np.array([[pred_id]])\n",
    "\n",
    "predicted_sentence = ' '.join([target_tokenizer.index_word[i] for i in predicted_sentence if i in target_tokenizer.index_word])\n",
    "reference_sentence = ' '.join([target_tokenizer.index_word[i] for i in y_test[0] if i in target_tokenizer.index_word])\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentence, predicted_sentence)\n",
    "perplexity = calculate_perplexity(total_loss / len(X_test))\n",
    "\n",
    "print(f'Predicted: {predicted_sentence}')\n",
    "print(f'Reference: {reference_sentence}')\n",
    "print(f'BLEU Score: {bleu_score}')\n",
    "print(f'Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
